{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/StarLord598/Interesting_Notebook_Exercises/blob/main/Andres_Version_Agentic_RAG_Hands_On.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#This hands on experiment will enable you to implement Search engine->LLM for accurate QA.\n",
        "https://medium.com/the-ai-forum/implementing-agentic-rag-using-langchain-b22af7f6a3b5"
      ],
      "metadata": {
        "id": "-tgb3u7R5yDt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "lUczVUYwHBjP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5c53c09-6a4a-458a-d4b2-d4bd0ab82a15"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m974.0/974.0 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.6/88.6 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m314.7/314.7 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.9/124.9 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m325.5/325.5 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.1/81.1 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m46.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.7/142.7 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.0/27.0 MB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m61.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.8/15.8 MB\u001b[0m \u001b[31m67.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -qU langchain langchain_openai langgraph arxiv duckduckgo-search\n",
        "!pip install -qU faiss-cpu pymupdf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade tiktoken"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GxJ0cJIT0_UM",
        "outputId": "07a20946-d6be-4d7d-9f2d-a54b89a09bd1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (0.7.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.5.15)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.6.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-community langchain-core"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "00dTJ9Xmy0aq",
        "outputId": "3f078ac8-0736-483d-c5e3-0ff9f5b261fd"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.2.4-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: langchain-core in /usr/local/lib/python3.10/dist-packages (0.2.5)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.0.30)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (3.9.5)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Requirement already satisfied: langchain<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.2.3)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.1.75)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (1.25.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (8.3.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core) (1.33)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core) (23.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain-core) (2.7.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (4.0.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading marshmallow-3.21.3-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.2/49.2 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core) (2.4)\n",
            "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from langchain<0.3.0,>=0.2.0->langchain-community) (0.2.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.0->langchain-community) (3.10.4)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain-core) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain-core) (2.18.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain-core) (4.12.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (2024.6.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.0.3)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: mypy-extensions, marshmallow, typing-inspect, dataclasses-json, langchain-community\n",
            "Successfully installed dataclasses-json-0.6.7 langchain-community-0.2.4 marshmallow-3.21.3 mypy-extensions-1.0.0 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# os.environ['OPENAI_API_KEY'] =  \"<YOUR KEY HERE>\""
      ],
      "metadata": {
        "id": "JstP9bCmv1gE"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "kx1fJ9NoLr-C"
      },
      "outputs": [],
      "source": [
        "# Step 1: Setup the environment and variables\n",
        "from google.colab import userdata\n",
        "from uuid import uuid4\n",
        "import os\n",
        "#\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = f\"AIE1 - LangGraph - {uuid4().hex[0:8]}\"\n",
        "\n",
        "# os.environ[\"LANGCHAIN_API_KEY\"] =  \"<YOUR KEY HERE>\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "EUaCfZig-sB4"
      },
      "outputs": [],
      "source": [
        "#Step 2: Simple Retrieval chain. This will error out if you have $0 credit for yout openai key\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import ArxivLoader\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "# Load the document pertaining to a particular topic\n",
        "docs = ArxivLoader(query=\"Retrieval Augmented Generation\", load_max_docs=2).load()\n",
        "\n",
        "# Split the dpocument into smaller chunks\n",
        "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
        "    chunk_size=100, chunk_overlap=10\n",
        ")\n",
        "\n",
        "chunked_documents = text_splitter.split_documents(docs)\n",
        "#\n",
        "# Instantiate the Embedding Model\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\",openai_api_key=os.environ['OPENAI_API_KEY'])\n",
        "# Create Index- Load document chunks into the vectorstore\n",
        "faiss_vectorstore = FAISS.from_documents(\n",
        "    documents=chunked_documents,\n",
        "    embedding=embeddings,\n",
        ")\n",
        "# Create a retriver\n",
        "retriever = faiss_vectorstore.as_retriever()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 3: generate RAG\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "RAG_PROMPT = \"\"\"\\\n",
        "Use the following context to answer the user's query. If you cannot answer the question, please respond with 'I don't know'.\n",
        "\n",
        "Question:\n",
        "{question}\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\"\"\"\n",
        "\n",
        "rag_prompt = ChatPromptTemplate.from_template(RAG_PROMPT)"
      ],
      "metadata": {
        "id": "aMvASHK8ynWv"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 4: Start the LLM\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "openai_chat_model = ChatOpenAI(model=\"gpt-3.5-turbo\")"
      ],
      "metadata": {
        "id": "gDeErzj40vfp"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 5: Build a RAG chain\n",
        "from operator import itemgetter\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "\n",
        "retrieval_augmented_generation_chain = (\n",
        "       {\"context\": itemgetter(\"question\")\n",
        "    | retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | openai_chat_model, \"context\": itemgetter(\"context\")}\n",
        ")\n",
        "#\n",
        "retrieval_augmented_generation_chain"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eyx_VQRG00ER",
        "outputId": "672db884-c37f-4c35-97a7-df286d885e79"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{\n",
              "  context: RunnableLambda(itemgetter('question'))\n",
              "           | VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x7d0ddd3ac370>),\n",
              "  question: RunnableLambda(itemgetter('question'))\n",
              "}\n",
              "| RunnableAssign(mapper={\n",
              "    context: RunnableLambda(itemgetter('context'))\n",
              "  })\n",
              "| {\n",
              "    response: ChatPromptTemplate(input_variables=['context', 'question'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template=\"Use the following context to answer the user's query. If you cannot answer the question, please respond with 'I don't know'.\\n\\nQuestion:\\n{question}\\n\\nContext:\\n{context}\\n\"))])\n",
              "              | ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7d0ddd2a3df0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7d0ddd2a1a20>, openai_api_key=SecretStr('**********'), openai_proxy=''),\n",
              "    context: RunnableLambda(itemgetter('context'))\n",
              "  }"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 6: Ask a Question\n",
        "await retrieval_augmented_generation_chain.ainvoke({\"question\" : \"Who wrote the paper on retrieval augmentation generation?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3s--fyAM06D4",
        "outputId": "ab0b3b56-cd7c-4d4b-c363-32d83c23bcee"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'response': AIMessage(content='Huayang Li, Yixuan Su, Deng Cai, Yan Wang, and Lemao Liu wrote the paper on retrieval augmentation generation.', response_metadata={'token_usage': {'completion_tokens': 29, 'prompt_tokens': 1226, 'total_tokens': 1255}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-1b0a1774-1dc4-4c70-8130-9c4f4ac18333-0', usage_metadata={'input_tokens': 1226, 'output_tokens': 29, 'total_tokens': 1255}),\n",
              " 'context': [Document(page_content='Retrieval-Augmented Paradigm\\nIn this section, we ﬁrst give a general formulation\\nof retrieval-augmented text generation. Then, we\\ndiscuss three major components of the retrieval-\\naugmented generation paradigm, including the re-\\narXiv:2202.01110v2  [cs.CL]  13 Feb 2022\\nInput\\nSources \\n(Sec. 2.2):\\nTraining \\nCorpus', metadata={'Published': '2022-02-13', 'Title': 'A Survey on Retrieval-Augmented Text Generation', 'Authors': 'Huayang Li, Yixuan Su, Deng Cai, Yan Wang, Lemao Liu', 'Summary': 'Recently, retrieval-augmented text generation attracted increasing attention\\nof the computational linguistics community. Compared with conventional\\ngeneration models, retrieval-augmented text generation has remarkable\\nadvantages and particularly has achieved state-of-the-art performance in many\\nNLP tasks. This paper aims to conduct a survey about retrieval-augmented text\\ngeneration. It firstly highlights the generic paradigm of retrieval-augmented\\ngeneration, and then it reviews notable approaches according to different tasks\\nincluding dialogue response generation, machine translation, and other\\ngeneration tasks. Finally, it points out some important directions on top of\\nrecent methods to facilitate future research.'}),\n",
              "  Document(page_content='This paper aims to review many representative\\napproaches for retrieval-augmented text generation\\ntasks including dialogue response generation (We-\\nston et al., 2018), machine translation (Gu et al.,\\n2018) and others (Hashimoto et al., 2018). We\\n∗All authors contributed equally.\\nﬁrstly present the generic paradigm of retrieval-\\naugmented generation as well as three key com-', metadata={'Published': '2022-02-13', 'Title': 'A Survey on Retrieval-Augmented Text Generation', 'Authors': 'Huayang Li, Yixuan Su, Deng Cai, Yan Wang, Lemao Liu', 'Summary': 'Recently, retrieval-augmented text generation attracted increasing attention\\nof the computational linguistics community. Compared with conventional\\ngeneration models, retrieval-augmented text generation has remarkable\\nadvantages and particularly has achieved state-of-the-art performance in many\\nNLP tasks. This paper aims to conduct a survey about retrieval-augmented text\\ngeneration. It firstly highlights the generic paradigm of retrieval-augmented\\ngeneration, and then it reviews notable approaches according to different tasks\\nincluding dialogue response generation, machine translation, and other\\ngeneration tasks. Finally, it points out some important directions on top of\\nrecent methods to facilitate future research.'}),\n",
              "  Document(page_content='et al., 2019), code generation (Hashimoto et al.,\\n2018), paraphrase (Kazemnejad et al., 2020; Su\\net al., 2021b), and knowledge-intensive generation\\n(Lewis et al., 2020b). Finally, we also point out\\nsome promising directions on retrieval-augmented\\ngeneration to push forward the future research.\\n2\\nRetrieval-Augmented Paradigm', metadata={'Published': '2022-02-13', 'Title': 'A Survey on Retrieval-Augmented Text Generation', 'Authors': 'Huayang Li, Yixuan Su, Deng Cai, Yan Wang, Lemao Liu', 'Summary': 'Recently, retrieval-augmented text generation attracted increasing attention\\nof the computational linguistics community. Compared with conventional\\ngeneration models, retrieval-augmented text generation has remarkable\\nadvantages and particularly has achieved state-of-the-art performance in many\\nNLP tasks. This paper aims to conduct a survey about retrieval-augmented text\\ngeneration. It firstly highlights the generic paradigm of retrieval-augmented\\ngeneration, and then it reviews notable approaches according to different tasks\\nincluding dialogue response generation, machine translation, and other\\ngeneration tasks. Finally, it points out some important directions on top of\\nrecent methods to facilitate future research.'}),\n",
              "  Document(page_content='DUETRAG: COLLABORATIVE RETRIEVAL-AUGMENTED\\nGENERATION\\nA PREPRINT\\nDian Jiao\\nZhejiang University\\njd_dcd@zju.edu.cn\\nLi Cai\\nZhejiang University\\n22321284@zju.edu.cn\\nJingsheng Huang\\nZhejiang University\\njingsheng@zju.edu.cn\\nWenqiao Zhang', metadata={'Published': '2024-05-12', 'Title': 'DuetRAG: Collaborative Retrieval-Augmented Generation', 'Authors': 'Dian Jiao, Li Cai, Jingsheng Huang, Wenqiao Zhang, Siliang Tang, Yueting Zhuang', 'Summary': \"Retrieval-Augmented Generation (RAG) methods augment the input of Large\\nLanguage Models (LLMs) with relevant retrieved passages, reducing factual\\nerrors in knowledge-intensive tasks. However, contemporary RAG approaches\\nsuffer from irrelevant knowledge retrieval issues in complex domain questions\\n(e.g., HotPot QA) due to the lack of corresponding domain knowledge, leading to\\nlow-quality generations. To address this issue, we propose a novel\\nCollaborative Retrieval-Augmented Generation framework, DuetRAG. Our\\nbootstrapping philosophy is to simultaneously integrate the domain fintuning\\nand RAG models to improve the knowledge retrieval quality, thereby enhancing\\ngeneration quality. Finally, we demonstrate DuetRAG' s matches with expert\\nhuman researchers on HotPot QA.\"})]}"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "await retrieval_augmented_generation_chain.ainvoke({\"question\" : \"What is Extreme RAG in the paper ERATTA in arxiv?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FzNrlvXF2xNe",
        "outputId": "72791b48-8ad1-4240-9ff4-6716c2b18488"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'response': AIMessage(content=\"I don't know.\", response_metadata={'token_usage': {'completion_tokens': 5, 'prompt_tokens': 1353, 'total_tokens': 1358}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-c5d380a2-0ee1-4196-bf47-37e397ff0f79-0', usage_metadata={'input_tokens': 1353, 'output_tokens': 5, 'total_tokens': 1358}),\n",
              " 'context': [Document(page_content='DuetRAG is not only to equip models with domain-specific knowledge but also to enable them to utilize external\\ndocuments to obtain answers when internal knowledge is uncertain. By complementing internal and external knowledge,\\nDuetRAG aims to enhance the robustness of the model. We provide a detailed description of the DuetRAG methodology\\narXiv:2405.13002v1  [cs.CL]  12 May 2024', metadata={'Published': '2024-05-12', 'Title': 'DuetRAG: Collaborative Retrieval-Augmented Generation', 'Authors': 'Dian Jiao, Li Cai, Jingsheng Huang, Wenqiao Zhang, Siliang Tang, Yueting Zhuang', 'Summary': \"Retrieval-Augmented Generation (RAG) methods augment the input of Large\\nLanguage Models (LLMs) with relevant retrieved passages, reducing factual\\nerrors in knowledge-intensive tasks. However, contemporary RAG approaches\\nsuffer from irrelevant knowledge retrieval issues in complex domain questions\\n(e.g., HotPot QA) due to the lack of corresponding domain knowledge, leading to\\nlow-quality generations. To address this issue, we propose a novel\\nCollaborative Retrieval-Augmented Generation framework, DuetRAG. Our\\nbootstrapping philosophy is to simultaneously integrate the domain fintuning\\nand RAG models to improve the knowledge retrieval quality, thereby enhancing\\ngeneration quality. Finally, we demonstrate DuetRAG' s matches with expert\\nhuman researchers on HotPot QA.\"}),\n",
              "  Document(page_content='DuetRAG: Collaborative Retrieval-Augmented Generation\\nA PREPRINT\\nand its performance on various datasets in Section 3. DuetRAG outperforms existing RAG and fine-tuned models on\\nthe HotPot QA dataset (Yang et al., 2018), presenting a new pipeline for QA.\\n3\\nRelated Work\\nRetrieval-Augmented Language Models Augmenting language models with relevant information obtained from', metadata={'Published': '2024-05-12', 'Title': 'DuetRAG: Collaborative Retrieval-Augmented Generation', 'Authors': 'Dian Jiao, Li Cai, Jingsheng Huang, Wenqiao Zhang, Siliang Tang, Yueting Zhuang', 'Summary': \"Retrieval-Augmented Generation (RAG) methods augment the input of Large\\nLanguage Models (LLMs) with relevant retrieved passages, reducing factual\\nerrors in knowledge-intensive tasks. However, contemporary RAG approaches\\nsuffer from irrelevant knowledge retrieval issues in complex domain questions\\n(e.g., HotPot QA) due to the lack of corresponding domain knowledge, leading to\\nlow-quality generations. To address this issue, we propose a novel\\nCollaborative Retrieval-Augmented Generation framework, DuetRAG. Our\\nbootstrapping philosophy is to simultaneously integrate the domain fintuning\\nand RAG models to improve the knowledge retrieval quality, thereby enhancing\\ngeneration quality. Finally, we demonstrate DuetRAG' s matches with expert\\nhuman researchers on HotPot QA.\"}),\n",
              "  Document(page_content='various external knowledge bases has been shown to significantly improve the performance of various NLP tasks,\\nincluding language modeling (Guu et al., 2020; Borgeaud et al., 2022; Shi et al., 2023; Lin et al., 2023) and open domain\\nquestion answering (Izacard et al., 2022; Zhang et al., 2024). RAG mainly adopts the \"retrieve then read\" paradigm.', metadata={'Published': '2024-05-12', 'Title': 'DuetRAG: Collaborative Retrieval-Augmented Generation', 'Authors': 'Dian Jiao, Li Cai, Jingsheng Huang, Wenqiao Zhang, Siliang Tang, Yueting Zhuang', 'Summary': \"Retrieval-Augmented Generation (RAG) methods augment the input of Large\\nLanguage Models (LLMs) with relevant retrieved passages, reducing factual\\nerrors in knowledge-intensive tasks. However, contemporary RAG approaches\\nsuffer from irrelevant knowledge retrieval issues in complex domain questions\\n(e.g., HotPot QA) due to the lack of corresponding domain knowledge, leading to\\nlow-quality generations. To address this issue, we propose a novel\\nCollaborative Retrieval-Augmented Generation framework, DuetRAG. Our\\nbootstrapping philosophy is to simultaneously integrate the domain fintuning\\nand RAG models to improve the knowledge retrieval quality, thereby enhancing\\ngeneration quality. Finally, we demonstrate DuetRAG' s matches with expert\\nhuman researchers on HotPot QA.\"}),\n",
              "  Document(page_content='Wenqiao Zhang\\nZhejiang University\\nwenqiaozhang@zju.edu.cn\\nSiliang Tang\\nZhejiang University\\nsiliang@zju.edu.cn\\nYueting Zhuang\\nZhejiang University\\nyzhuang@zju.edu.cn\\nMay 24, 2024\\n1\\nAbstract\\nRetrieval-Augmented Generation (RAG) methods augment the input of Large Language Models (LLMs) with relevant', metadata={'Published': '2024-05-12', 'Title': 'DuetRAG: Collaborative Retrieval-Augmented Generation', 'Authors': 'Dian Jiao, Li Cai, Jingsheng Huang, Wenqiao Zhang, Siliang Tang, Yueting Zhuang', 'Summary': \"Retrieval-Augmented Generation (RAG) methods augment the input of Large\\nLanguage Models (LLMs) with relevant retrieved passages, reducing factual\\nerrors in knowledge-intensive tasks. However, contemporary RAG approaches\\nsuffer from irrelevant knowledge retrieval issues in complex domain questions\\n(e.g., HotPot QA) due to the lack of corresponding domain knowledge, leading to\\nlow-quality generations. To address this issue, we propose a novel\\nCollaborative Retrieval-Augmented Generation framework, DuetRAG. Our\\nbootstrapping philosophy is to simultaneously integrate the domain fintuning\\nand RAG models to improve the knowledge retrieval quality, thereby enhancing\\ngeneration quality. Finally, we demonstrate DuetRAG' s matches with expert\\nhuman researchers on HotPot QA.\"})]}"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# So we see our LLM has limited knowledge that needs to be enhanced"
      ],
      "metadata": {
        "id": "h0p6Cmiu3Sa2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 7: Creating a Tool belt with DuckduckGo web serach and arxiv\n",
        "from langchain_community.tools.ddg_search import DuckDuckGoSearchRun\n",
        "from langchain_community.tools.arxiv.tool import ArxivQueryRun\n",
        "from langgraph.prebuilt import ToolExecutor\n",
        "tool_belt = [\n",
        "    DuckDuckGoSearchRun(),\n",
        "    ArxivQueryRun()\n",
        "]\n",
        "\n",
        "tool_executor = ToolExecutor(tool_belt)"
      ],
      "metadata": {
        "id": "6w64Mk4i208x"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 8: Instantiate openai function calling\n",
        "from langchain_core.utils.function_calling import convert_to_openai_function\n",
        "#\n",
        "model = ChatOpenAI(temperature=0)\n",
        "#\n",
        "functions = [convert_to_openai_function(t) for t in tool_belt]\n",
        "model = model.bind_functions(functions)"
      ],
      "metadata": {
        "id": "Yw6KoOlw3ZW0"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 9: Instantiate Langgraph to pass information between various stages of the nodes\n",
        "from typing import TypedDict, Annotated, Sequence\n",
        "import operator\n",
        "from langchain_core.messages import BaseMessage\n",
        "\n",
        "class AgentState(TypedDict):\n",
        "  messages: Annotated[Sequence[BaseMessage], operator.add]"
      ],
      "metadata": {
        "id": "VWtgSGNY3zDo"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 10: Build Nodes, call_model and call_tool\n",
        "from langgraph.prebuilt import ToolInvocation\n",
        "import json\n",
        "from langchain_core.messages import FunctionMessage\n",
        "\n",
        "def call_model(state):\n",
        "  messages = state[\"messages\"]\n",
        "  response = model.invoke(messages)\n",
        "  return {\"messages\" : [response]}\n",
        "\n",
        "def call_tool(state):\n",
        "  last_message = state[\"messages\"][-1]\n",
        "\n",
        "  action = ToolInvocation(\n",
        "      tool=last_message.additional_kwargs[\"function_call\"][\"name\"],\n",
        "      tool_input=json.loads(\n",
        "          last_message.additional_kwargs[\"function_call\"][\"arguments\"]\n",
        "      )\n",
        "  )\n",
        "\n",
        "  response = tool_executor.invoke(action)\n",
        "\n",
        "  function_message = FunctionMessage(content=str(response), name=action.tool)\n",
        "\n",
        "  return {\"messages\" : [function_message]}"
      ],
      "metadata": {
        "id": "eYOKuXJH3_tq"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 11: Built workflow and set agent\n",
        "from langgraph.graph import StateGraph, END\n",
        "\n",
        "workflow = StateGraph(AgentState)\n",
        "\n",
        "workflow.add_node(\"agent\", call_model)\n",
        "workflow.add_node(\"action\", call_tool)\n",
        "workflow.set_entry_point(\"agent\")\n"
      ],
      "metadata": {
        "id": "kuiqsezS4H-1"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 12: Build a conditional edge for routing\n",
        "def should_continue(state):\n",
        "  last_message = state[\"messages\"][-1]\n",
        "\n",
        "  if \"function_call\" not in last_message.additional_kwargs:\n",
        "    return \"end\"\n",
        "\n",
        "  return \"continue\"\n",
        "\n",
        "workflow.add_conditional_edges(\n",
        "    \"agent\",\n",
        "    should_continue,\n",
        "    {\n",
        "        \"continue\" : \"action\",\n",
        "        \"end\" : END\n",
        "    }\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "ePAjINzg4RpU"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#connect conditional edge to agent\n",
        "workflow.add_edge(\"action\", \"agent\")"
      ],
      "metadata": {
        "id": "7qy9iGvC4Ydc"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 13: Compile the graph\n",
        "app = workflow.compile()\n",
        "app"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K4LXTncm4nXw",
        "outputId": "99fd5f61-1bb4-4b00-fb53-99c03486218f"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CompiledStateGraph(nodes={'__start__': PregelNode(config={'tags': ['langsmith:hidden']}, channels=['__start__'], triggers=['__start__'], writers=[ChannelWrite<messages>(recurse=True, writes=[ChannelWriteEntry(channel='messages', value=<object object at 0x7d0de2af8330>, skip_none=False, mapper=_get_state_key(recurse=False))], require_at_least_one_of=['messages']), ChannelWrite<start:agent>(recurse=True, writes=[ChannelWriteEntry(channel='start:agent', value='__start__', skip_none=False, mapper=None)], require_at_least_one_of=None)]), 'agent': PregelNode(config={'tags': []}, channels={'messages': 'messages'}, triggers=['start:agent', 'action'], mapper=functools.partial(<function _coerce_state at 0x7d0ddd0c5ab0>, <class '__main__.AgentState'>), writers=[ChannelWrite<agent,messages>(recurse=True, writes=[ChannelWriteEntry(channel='agent', value='agent', skip_none=False, mapper=None), ChannelWriteEntry(channel='messages', value=<object object at 0x7d0de2af8330>, skip_none=False, mapper=_get_state_key(recurse=False))], require_at_least_one_of=['messages']), _route(recurse=True, _is_channel_writer=True)]), 'action': PregelNode(config={'tags': []}, channels={'messages': 'messages'}, triggers=['branch:agent:should_continue:action'], mapper=functools.partial(<function _coerce_state at 0x7d0ddd0c5ab0>, <class '__main__.AgentState'>), writers=[ChannelWrite<action,messages>(recurse=True, writes=[ChannelWriteEntry(channel='action', value='action', skip_none=False, mapper=None), ChannelWriteEntry(channel='messages', value=<object object at 0x7d0de2af8330>, skip_none=False, mapper=_get_state_key(recurse=False))], require_at_least_one_of=['messages'])])}, channels={'messages': <langgraph.channels.binop.BinaryOperatorAggregate object at 0x7d0ddc4e32b0>, '__start__': <langgraph.channels.ephemeral_value.EphemeralValue object at 0x7d0ddc4e36a0>, 'agent': <langgraph.channels.ephemeral_value.EphemeralValue object at 0x7d0ddc4e39d0>, 'action': <langgraph.channels.ephemeral_value.EphemeralValue object at 0x7d0ddc4e1810>, 'start:agent': <langgraph.channels.ephemeral_value.EphemeralValue object at 0x7d0ddc4e3370>, 'branch:agent:should_continue:action': <langgraph.channels.ephemeral_value.EphemeralValue object at 0x7d0ddc4e3d60>}, auto_validate=False, stream_mode='updates', output_channels=['messages'], stream_channels=['messages'], input_channels='__start__', builder=<langgraph.graph.state.StateGraph object at 0x7d0ddc4e3940>)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 14: Invoke Ask a Query\n",
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "inputs = {\"messages\" : [HumanMessage(content=\"Who wrote the paper ERATTA in arxiv?\")]}\n",
        "\n",
        "response = app.invoke(inputs)\n",
        "print(response['messages'][-1].content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "--jqrGUv4ud0",
        "outputId": "79c56c18-980d-4c21-c2a1-6a678e05397c"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The paper \"ERATTA: Extreme RAG for Table To Answers with Large Language Models\" on arXiv was written by Sohini Roychowdhury, Marko Krema, Anvar Mahammad, Brian Moore, Arijit Mukherjee, and Punit Prakashchandra.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Vary teh question\n",
        "question = \"What is Extreme RAG in the paper ERATTA in arxiv?\"\n",
        "\n",
        "inputs = {\"messages\" : [HumanMessage(content=question)]}\n",
        "#\n",
        "response = app.invoke(inputs)\n",
        "print(response['messages'][-1].content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wDB8AgPU44DW",
        "outputId": "575a7d51-df3c-4bf0-bc69-dd2b8c0ca9d2"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The paper \"ERATTA: Extreme RAG for Table To Answers with Large Language Models\" proposes a unique Large Language Model (LLM)-based system where multiple LLMs can be invoked to enable data authentication, user query routing, data retrieval, and custom prompting for question-answering capabilities from data tables that are highly varying and large in size. The system is tuned to extract information from Enterprise-level data products and provide real-time responses under 10 seconds. It includes a five metric scoring module that detects and reports hallucinations in the LLM responses. The proposed system and scoring metrics achieve >90% confidence scores across hundreds of user queries in sustainability, financial health, and social media domains. Extensions to the proposed extreme RAG architectures can enable heterogeneous source querying using LLMs.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sS62eT2-5Xa3"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}